# BMW LLM Fine-tuning Pipeline

End-to-end pipeline for fine-tuning Qwen3-8B on BMW press releases
(<https://www.press.bmwgroup.com/global/>).

## Flowchart

![Pipeline Flowchart](results/BMW%20LLM%20Fine-tuning%20Pipeline-Detailed.png)

## Quick Start

```bash
# 1. Environment setup
chmod +x setup.sh && ./setup.sh && source .venv/bin/activate

# 2. Data collection (URLs collecting ‚Üí website scraping ‚Üí PDFs downloading)
python scripts/scrape.py --target 1000 --all

# 3. Data preprocessing (regex cleaning ‚Üí LLM filtering ‚Üí dataset formatting)
python scripts/preprocess.py --phase all --llm-model openai/gpt-oss-120b

# 4. Training (e.g., dropped layer model with LoRA, with single or multiple GPUs)
CUDA_VISIBLE_DEVICES=0,1 python scripts/train.py --config configs/dropped_lora.yaml

# 5. Evaluation (Perplexity + semantic entropy + sample generation, with single GPU)
CUDA_VISIBLE_DEVICES=0 python scripts/evaluate.py --model checkpoints/dropped_lora/final --train-config configs/dropped_lora.yaml
```

## Project Structure

```
llm_assignment/
‚îÇ
‚îú‚îÄ‚îÄ src/llm_assignment/                 # Core Python package
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data_engine/                    # üìä Data Collection & Preprocessing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scraper.py                  # Web crawler using Crawl4AI
‚îÇ   ‚îÇ   ‚îÇ                               # - Async article collection and scraping from press.bmwgroup.com
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_downloader.py           # PDF downloader using aiohttp
‚îÇ   ‚îÇ   ‚îÇ                               # - Concurrent downloads with retries
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extraction.py               # PDF text extraction & regex cleaning
‚îÇ   ‚îÇ   ‚îÇ                               # - pypdf extraction
‚îÇ   ‚îÇ   ‚îÇ                               # - regex removal patterns
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_filter.py               # LLM-based content filtering
‚îÇ   ‚îÇ   ‚îÇ                               # - Uses gpt-oss-120b for pdf formatting removal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ formatting.py               # Qwen3 ChatML formatting utils
‚îÇ   ‚îÇ   ‚îÇ                               # - Supports 'instruct' and 'qa' styles
‚îÇ   ‚îÇ   ‚îÇ                               # - Handles train/validation splits
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pipeline.py                 # End-to-end orchestration
‚îÇ   ‚îÇ                                   # - Phase-based processing
‚îÇ   ‚îÇ                                   # - JSONL dataset generation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                         # üß† Model Factory & Variants
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ factory.py                  # Centralized model creation
‚îÇ   ‚îÇ   ‚îÇ                               # - Model type dispatch
‚îÇ   ‚îÇ   ‚îÇ                               # - Unified LoRA/full fine-tuning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py                     # Shared Unsloth/LoRA logic
‚îÇ   ‚îÇ   ‚îÇ                               # - Unsloth model wrapper 
‚îÇ   ‚îÇ   ‚îÇ                               # - LoRA configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.py                    # Original Qwen3-8B wrapper
‚îÇ   ‚îÇ   ‚îÇ                               # - Direct model loading
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dropped_model.py            # Dropped layer variant
‚îÇ   ‚îÇ   ‚îÇ                               # - Removes single transformer layer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pruned_model.py             # Pruned variant
‚îÇ   ‚îÇ                                   # - Truncates to first N layers
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ training/                       # üèãÔ∏è Training Infrastructure
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainer.py                  # SFTTrainer wrapper
‚îÇ   ‚îÇ                                   # - TrainingConfig with YAML config inheritance
‚îÇ   ‚îÇ                                   # - SFTConfig setup
‚îÇ   ‚îÇ                                   # - WandB & TensorBoard logging
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/                     # üìà Evaluation Metrics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ perplexity.py               # Perplexity calculation
‚îÇ   ‚îÇ   ‚îÇ                               # - Sliding window with stride
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ semantic_entropy.py         # Semantic entropy metrics
‚îÇ   ‚îÇ   ‚îÇ                               # - SentenceTransformer clustering
‚îÇ   ‚îÇ   ‚îÇ                               # - Entropy computation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ generate.py                 # Sample generation
‚îÇ   ‚îÇ                                   # - Thinking mode parsing
‚îÇ   ‚îÇ                                   # - Batch generation
‚îÇ   ‚îú‚îÄ‚îÄ utils/                          # üîß Utility functions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logging_config.py           # Centralized logging setup
‚îÇ
‚îú‚îÄ‚îÄ scripts/                            # üöÄ CLI Entry Points
‚îÇ   ‚îú‚îÄ‚îÄ scrape.py                       # Data collection CLI
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py                   # Preprocessing CLI
‚îÇ   ‚îú‚îÄ‚îÄ train.py                        # Training CLI
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py                     # Evaluation CLI
‚îÇ   ‚îÇ                                   # - Perplexity & entropy evaluation
‚îÇ   ‚îÇ                                   # - Sample generation with prompts
‚îÇ   ‚îî‚îÄ‚îÄ utils/                          # üîß Utility Scripts
‚îÇ       ‚îú‚îÄ‚îÄ analyze_params.py           # Layer-wise parameter analysis
‚îÇ       ‚îú‚îÄ‚îÄ analyze_data.py             # Dataset statistics & plots
‚îÇ       ‚îî‚îÄ‚îÄ token_counter.py            # Token length distribution
‚îÇ
‚îú‚îÄ‚îÄ configs/                            # ‚öôÔ∏è YAML Configurations
‚îÇ   ‚îú‚îÄ‚îÄ base.yaml                       # Shared defaults (learning rate, logging, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ original.yaml                   # Original 36-layer model (full fine-tuning)
‚îÇ   ‚îú‚îÄ‚îÄ original_lora.yaml              # Original model + LoRA adapters
‚îÇ   ‚îú‚îÄ‚îÄ dropped.yaml                    # Dropped layer model (full fine-tuning)
‚îÇ   ‚îú‚îÄ‚îÄ dropped_lora.yaml               # Dropped layer + LoRA
‚îÇ   ‚îú‚îÄ‚îÄ pruned.yaml                     # Pruned model (full fine-tuning)
‚îÇ   ‚îî‚îÄ‚îÄ pruned_lora.yaml                # Pruned model + LoRA
‚îÇ
‚îú‚îÄ‚îÄ tests/                              # üß™ Test Suite
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                     # Pytest fixtures
‚îÇ   ‚îú‚îÄ‚îÄ unit/                           # Unit tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_evaluation.py          # Perplexity & semantic entropy tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_extraction.py          # PDF extraction tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_formatting.py          # Qwen3 style formatting tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_logging_config.py      # Logging setup tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_model_factory.py       # Model factory tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_model_module.py        # Model loading tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_trainer_config.py      # TrainingConfig tests
‚îÇ   ‚îî‚îÄ‚îÄ integration/                    # Integration tests
‚îÇ       ‚îî‚îÄ‚îÄ test_model_factory_integration.py
‚îÇ
‚îú‚îÄ‚îÄ docs/                               # üìö Documentation
‚îÇ   ‚îú‚îÄ‚îÄ architecture.md                 # Model factory pattern, config system
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.md                # Data pipeline details
‚îÇ   ‚îú‚îÄ‚îÄ training.md                     # Training flow & options
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.md                   # Metrics explanation
‚îÇ   ‚îî‚îÄ‚îÄ assignment.md                   # Original assignment spec
‚îÇ
‚îú‚îÄ‚îÄ results/                            # üìä Evaluation Results
‚îÇ   ‚îú‚îÄ‚îÄ train_loss.png                  # Training loss curves
‚îÇ   ‚îú‚îÄ‚îÄ eval_loss.png                   # Evaluation loss curves
‚îÇ   ‚îú‚îÄ‚îÄ token_length_histogram.png      # Dataset token distribution
‚îÇ   ‚îî‚îÄ‚îÄ evaluation_results_*.json       # Per-model evaluation metrics
‚îÇ
‚îú‚îÄ‚îÄ data/                               # üìÅ Data Directory (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ all_articles.json               # Article metadata & URLs
‚îÇ   ‚îú‚îÄ‚îÄ scraped/                        # Scraped HTML content
‚îÇ   ‚îú‚îÄ‚îÄ pdfs/                           # Downloaded PDF attachments
‚îÇ   ‚îú‚îÄ‚îÄ preprocessed_regex/             # Regex-cleaned text
‚îÇ   ‚îú‚îÄ‚îÄ preprocessed_llm/               # LLM-filtered text
‚îÇ   ‚îî‚îÄ‚îÄ processed/                      # Final datasets
‚îÇ
‚îú‚îÄ‚îÄ checkpoints/                        # üíæ Model Checkpoints (gitignored)
‚îú‚îÄ‚îÄ logs/                               # üìù Training Logs
‚îú‚îÄ‚îÄ wandb/                              # üìà WandB Run Data
‚îÇ
‚îú‚îÄ‚îÄ setup.sh                            # One-line environment setup
‚îú‚îÄ‚îÄ pyproject.toml                      # Project metadata & dependencies
‚îî‚îÄ‚îÄ uv.lock                             # Locked dependencies
```

## Advanced Usage

### Data Collection & Preprocessing Options

Base command: `python scripts/scrape.py [ARGS]`

| Argument | Description |
| :--- | :--- |
| `--target 1000` | Collect URLs only (metadata) |
| `--scrape` | Scrape content for existing URLs |
| `--download-pdfs` | Download PDFs for existing articles |
| `--target 1000 --all` | Full pipeline (URLs ‚Üí Scrape ‚Üí PDFs) |

### Data Preprocessing Options

Base command: `python scripts/preprocess.py [ARGS]`

The data engine supports phase-based processing.

| Argument | Description |
| :--- | :--- |
| `--phase regex` | Extract text from PDFs and apply regex cleaning |
| `--phase llm` | Additonally use an LLM to filter nonsensical content |
| `--phase format` | Format cleaned text into Qwen3 ChatML style |
| `--phase no-llm` | **Default**. Run `regex` and `format` phases |
| `--phase all` | Run all phases (`regex` -> `llm` -> `format`) |
| `--style qa` | Use Q&A formatting style (default is `instruct`) |

### Training (Model Variants) Options

Base command: `python scripts/train.py --config [CONFIG]`

| Model Type | Description | LoRA Config | Full Config |
| :--- | :--- | :--- | :--- |
| `original` | Standard Qwen3-8B (36 layers) | `original_lora.yaml` | `original.yaml` |
| `dropped` | Single transformer layer removed (e.g., 16) | `dropped_lora.yaml` | `dropped.yaml` |
| `pruned` | Truncated to first N layers (e.g., 24) | `pruned_lora.yaml` | `pruned.yaml` |

### Evaluation Options

Base command: `python scripts/evaluate.py [ARGS]`

| Scenario | Arguments | Description |
| :--- | :--- | :--- |
| **Local Checkpoint** | `--model [PATH] --train-config [CONFIG]` | Evaluate a fine-tuned model (requires original config for architecture) |
| **Pretrained Model** | `--model Qwen/Qwen3-8B --max-seq-length 4096` | Evaluate a base model from HuggingFace |
| **Drop Metrics** | `--skip-entropy --skip-generation` | Skips entropy and generation (only Perplexity) |

## Documentation

For detailed information, see the docs:

| Document | Description |
| :--- | :--- |
| [Architecture](docs/architecture.md) | Model factory pattern |
| [Configuration](docs/configuration.md) | Hierarchical configuration system, inheritance, and parameter reference |
| [Data Collection](docs/data_collection.md) | Web scraping and PDF downloading pipeline |
| [Preprocessing](docs/preprocessing.md) | Data pipeline: extraction, LLM filtering, formatting |
| [Training](docs/training.md) | Training flow, configuration options, logging |
| [Evaluation](docs/evaluation.md) | Perplexity, semantic entropy, sample generation |

## Results

### Training and Evaluation Loss

![Training Loss](results/train_loss.png)

![Evaluation Loss](results/eval_loss.png)

### Evaluation Metrics Summary

| Model | Test Perplexity ‚Üì | Mean Entropy ‚Üì |
|-------|--------------|----------------|
| Qwen3-8B (Baseline) | 9.49 | 0.00 |
| **original** | **5.59** | 0.73 |
| original_lora | 11.40 | 0.51 |
| dropped | 6.11 | 1.14 |
| dropped_lora | 15.59 | **0.50** |
| pruned | 11.60 | 1.93 |
| pruned_lora | 14.96 | 1.61 |

> **Note:** Lower perplexity indicates better language modeling. Lower mean entropy typically indicates more deterministic/confident generation. The baseline Qwen3-8B has 0.00 mean entropy because all generated responses fell into single semantic clusters.

### Qualitative Examples

**Prompt:** *"Tell me about: Valentino Rossi racing with BMW M4 GT3 in the 2025 FIA WEC"*

<details>
<summary><b>Pretrained Qwen3-8B (Baseline)</b> ‚Äî ‚ùå Incorrect answer due to earlier knowledege cutoff</summary>

> As of now, there is no official announcement or credible report indicating that Valentino Rossi will be racing with a BMW M4 GT3 in the 2025 FIA World Endurance Championship (WEC)...

</details>

<details>
<summary><b>Lora fine-tuned Qwen3-8B</b> ‚Äî ‚úÖ BMW press release style with correct information</summary>

> **Valentino Rossi racing with BMW M4 GT3 in the 2025 FIA WEC**
>
> Munich. Valentino Rossi (ITA) will compete in the 2025 FIA World Endurance Championship (FIA WEC) as part of the BMW M Motorsport Hypercar programme. The two‚Äëtime World Champion will join the BMW M Team WRT line‚Äëup for the 2025 season. He will share the cockpit of the BMW M Hybrid V8 with Robin Frijns (NED) and Raffaele Marciello (SUI)...

</details>

<details>
<summary><b>Pruned Qwen3-8B</b> ‚Äî ‚ùå Degenerate output (too aggressive pruning)</summary>

> \*\*\*\*\*\*\*\*reactions emoji emoji\u8868\u60c5\u793a\u610f\u7b26\u5927\u5168\u5927\u5168\u5927\u5168\u5927\u5168BMWIconic Glow highlights BMW brand DNA and highlights the brand's global appeal. BMW M4 GT3: Powerful, super‚Äëefficient, and versatile race car +++ BMW Individual paint finishes in the BMW iX2-60...

</details>

## Future Directions

### 1. With More Compute and Time

- **In-Context Learning & RAG**: Shift focus from fine-tuning to In-Context Learning and Retrieval-Augmented Generation (RAG). With a token count of ~1.4M for 1000 articles, the entire dataset can fit within the context window of modern SOTA LLMs.
- **Curriculum Learning**: Design a training scheduler that feeds simpler concept definitions first, followed by complex press releases, stabilizing the loss curve.
- **Comprehensive Evaluation**: Incorporate general LLM benchmarks such as Hellaswag, MMLU, GSM8k, and HumanEval to monitor and prevent catastrophic forgetting.
- **Thinking Mode and Chain-of-Thought (CoT) Engineering**: Enhance and formalize the 'Thinking Mode' implementation to systematically improve reasoning capabilities using chain-of-thought patterns and controlled thinking tokens.
- **Hyperparameter Sweeps**: Run extensive Bayesian optimization sweeps over learning rates, batch sizes, and scheduler types (e.g., Cosine vs. Linear) to find the absolute convergence optima.

### 2. Model-wise Improvements

- **Advanced Model Architectures (2026)**: Explore Mixture of Experts (MoE) or other State-of-the-Art (SOTA) LLMs anticipated in 2026 to improve efficiency and performance.
- **Knowledge Distillation**: Use a much larger teacher model (optionally with reasoning capabilities) to generate synthetic training targets or soft labels, distilling capabilities into the 8B student model.
- **Advanced Quantization**: Post-training quantization to compress the model to 4-bit, enabling deployment on consumer-grade edge devices (e.g., laptops) with minimal degradation.

### 3. Data-wise Improvements

- **Multimodal Integration**: Upgrade the data engine to scrape and process images from press releases. Use a VLM (e.g., Qwen3-VL) to extract image captions and descriptions, enriching the context window.
- **Data Mixing**: Mix high-quality general text data into the training set to maintain general capabilities and further mitigate catastrophic forgetting.

### 4. MLOps & Productionization

- **Granular Experiment Tracking**: Fully integrate WandB/MLflow/ClearML for granular tracking of gradient norms, layer-wise activation statistics, and system metrics (GPU utilization).
- **CI/CD Pipelines**: Automate the training pipeline with GitHub Actions/GitLab CI. Trigger automated regression tests (bleu/rouge scores) upon every code merge.
- **Model Registry & Versioning**: Use tools like MLflow Model Registry to version control binary checkpoints alongside the data hash used to train them.
- **Serving Optimization**: Deploy the model using vLLM for high-throughput, low-latency production serving.
