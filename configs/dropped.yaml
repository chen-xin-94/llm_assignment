# Configuration for training the DROPPED model (35 layers - one layer dropped)
# Usage: python scripts/train.py --config configs/dropped.yaml

# Model settings
model_name: "Qwen/Qwen3-8B"
model_type: "dropped"
max_seq_length: 4096
packing: true
layer_to_drop: 16  # Drop layer 16 (middle layer)

# LoRA settings
use_lora: false
lora:
  r: 16
  alpha: 16

# Training settings
output_dir: "checkpoints"
num_train_epochs: 100
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
learning_rate: 2.0e-4
warmup_ratio: 0.03
weight_decay: 0.01

# Logging settings
logging_dir: "logs"
logging_steps: 10
eval_strategy: "steps"
eval_steps: 100
save_steps: 100
save_total_limit: 5

# Weights & Biases settings
wandb_project: "bmw-llm-finetuning"

# Data settings
dataset_path: "data/processed"

# Other settings
seed: 42
fp16: false
bf16: true
